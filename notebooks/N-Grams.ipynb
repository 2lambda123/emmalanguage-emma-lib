{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.resolvers() = interp.resolvers() :+ ammonite.runtime.tools.Resolver.File(\n",
    "  \"m2\",\n",
    "  \"/.m2/repository\",\n",
    "  \"/[organisation]/[module]/[revision]/[artifact]-[revision].[ext]\",\n",
    "  m2 = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mammonite.ops._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.Paths\n",
       "\n",
       "// required spark dependencies\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                         \n",
       "// required emma dependencies\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                         \n",
       "\n",
       "// project classpath\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ammonite.ops._\n",
    "import java.nio.file.Paths\n",
    "\n",
    "// required spark dependencies\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`edu.berkeley.cs.amplab::keystoneml:0.4.0`\n",
    "// required emma dependencies\n",
    "import $ivy.`org.emmalanguage:emma-language:0.2-SNAPSHOT`\n",
    "import $ivy.`org.emmalanguage:emma-spark:0.2-SNAPSHOT`\n",
    "\n",
    "// project classpath\n",
    "interp.load.cp(pwd / up / \"emma-lib\" / \"target\" / \"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Dataset\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Encoder\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.explode\n",
       "// spark-mllib\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.RegexTokenizer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.Tokenizer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.NGram\n",
       "// emma-lib-base\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.data.reddit.Comment\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.api._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.api.Meta.Projections._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.api.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.lib.ml._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.emmalanguage.lib.ml.feature._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-sql\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Encoder\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.explode\n",
    "// spark-mllib\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.feature.NGram\n",
    "// emma-lib-base\n",
    "import org.emmalanguage.data.reddit.Comment\n",
    "import org.emmalanguage.api._\n",
    "import org.emmalanguage.api.Meta.Projections._\n",
    "import org.emmalanguage.api.spark._\n",
    "import org.emmalanguage.lib.ml._\n",
    "import org.emmalanguage.lib.ml.feature._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/alexander/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/alexander/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4753b042"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// implicit Spark environment\n",
    "implicit val spark = SparkSession.builder()\n",
    "    .appName(\"N-Gram transformation\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", Paths.get(sys.props(\"java.io.tmpdir\"), \"spark-warehouse\").toUri.toString)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mreddit\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mComment\u001b[39m] = [author: string, author_flair_css_class: string ... 16 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reddit = spark.read\n",
    "    .schema(implicitly[Encoder[Comment]].schema)\n",
    "    .json(\"file:///data/alexander/reddit/bz2/RC_2006-01.bz2\")\n",
    "    .as[Comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mRegexTokenizer\u001b[39m = regexTok_1c02b008d3f8\n",
       "\u001b[36mtokenized\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [author: string, author_flair_css_class: string ... 17 more fields]\n",
       "\u001b[36mres6_2\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m])] = \u001b[33mArray\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m\"I have nothing but good things to say about Dell Tech Support. Many a time I've called in a faulty part and had the replacement at the front door in two days with a box to ship the old one back. First class service if you ask me.\"\u001b[39m,\n",
       "    \u001b[33mArray\u001b[39m(\n",
       "      \u001b[32m\"i\"\u001b[39m,\n",
       "      \u001b[32m\"have\"\u001b[39m,\n",
       "      \u001b[32m\"nothing\"\u001b[39m,\n",
       "      \u001b[32m\"but\"\u001b[39m,\n",
       "      \u001b[32m\"good\"\u001b[39m,\n",
       "      \u001b[32m\"things\"\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"body\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "val tokenized = tokenizer.transform(reddit.filter($\"id\" === \"c2722\"))\n",
    "tokenized.select(\"body\", \"tokens\").as[(String, Array[String])].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mxs\u001b[39m: \u001b[32mDataBag\u001b[39m[\u001b[32mComment\u001b[39m] = org.emmalanguage.api.SparkDataset@9c8c8073\n",
       "\u001b[36mrs\u001b[39m: \u001b[32mDataBag\u001b[39m[\u001b[32mFPoint\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]]] = org.emmalanguage.api.SparkDataset@945cabed"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xs = DataBag.from(reddit)\n",
    "val rs = emma.onSpark {\n",
    "    split.gaps[Comment, String]()(_.id, _.body)(xs.withFilter(_.id == \"c2722\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mFPoint\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m]]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mFPoint\u001b[39m(\n",
       "    \u001b[32m\"c2722\"\u001b[39m,\n",
       "    \u001b[33mArray\u001b[39m(\n",
       "      \u001b[32m\"I\"\u001b[39m,\n",
       "      \u001b[32m\"have\"\u001b[39m,\n",
       "      \u001b[32m\"nothing\"\u001b[39m,\n",
       "      \u001b[32m\"but\"\u001b[39m,\n",
       "      \u001b[32m\"good\"\u001b[39m,\n",
       "      \u001b[32m\"things\"\u001b[39m,\n",
       "      \u001b[32m\"to\"\u001b[39m,\n",
       "      \u001b[32m\"say\"\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ngramXfrm = new NGram().setN(3).setInputCol(\"body\").setOutputCol(\"ngrams\")\n",
    "val ngrams = ngramXfrm.transform(reddit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
